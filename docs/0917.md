### Meeting on 17.September

During this time I have organized my task.

##### Thesis objectives

1. Using cross-modal model to classify the relation between image and text.
2. Gernation of congrant/incongrent datasets
3. Generating auxinliary informations(object, action, caption) to research if it is helpful for relation classification.
   
##### Model of task

- For this thesis task I think 3 model types need to be used.

1. Model for incongruence data generation:
   1. Use CLIP model
      1.  Validate if pre-trained CLIP model is avaiable by evaluating the accuracy of positive pairs.
      2.  Generating negative pairs by choosing the minimal similarity.
2. Model for auxiliary information extraction:
   1. Object detection model: Yolo
   2. Action detection model: ?
   3. Caption genaration model: ?
3. Model for image-text relation classification:
   1. Based on paper Alikhani et al.2022, instead image encoder Resnet50 by using Visual Transformer; instead text encoder LSTM + attention-based layer by using transformer-based model Bert/Llama.

##### Experiment settings
- Using auxiliary information to with classification model to analyze the result and compare the similarity:
  1. baseline: image + text
  2. auxiliary information(caption) + text: simplify model, reduce model parameter
  3. image/auxiliary information concate + text: increase accuracy
  4. image/auxiliary weighted + text: increase accuracy

##### Important thing!!!
Finish proposal ASAP! 

image-to-txt: blip, git, gpt2, llama, llava
- computer vision/multimodal in huggingface
